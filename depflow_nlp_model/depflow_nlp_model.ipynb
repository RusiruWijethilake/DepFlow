{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CA_GIfeB-Iz",
    "outputId": "459b7864-d9b1-4bd1-ca56-65a42babcc23"
   },
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu_Zd80bCO9j"
   },
   "outputs": [],
   "source": [
    "vocab_size = 2170\n",
    "embedding_dim = 300\n",
    "max_length = 20\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 700\n",
    "num_epochs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj1NXA86Cb_6"
   },
   "outputs": [],
   "source": [
    "#All of the sinhala chars availble in the current Sinhala keyboard.\n",
    "sinhalaChars = [\"අ\", \"ආ\", \"ඇ\", \"ඈ\", \"ඉ\", \"ඊ\",\"උ\", \"ඌ\", \"ඍ\", \"ඎ\", \"ඏ\", \"ඐ\",\"එ\", \"ඒ\", \"ඓ\", \"ඔ\", \"ඕ\", \"ඖ\",\"ං\", \"ඃ\",\n",
    "\"ක\", \"ඛ\", \"ග\", \"ඝ\", \"ඞ\", \"ඟ\",\"ච\", \"ඡ\", \"ජ\", \"ඣ\", \"ඤ\", \"ඥ\", \"ඦ\",\"ට\", \"ඨ\", \"ඩ\", \"ඪ\", \"ණ\", \"ඬ\",\"ත\", \"ථ\", \"ද\", \"ධ\",\n",
    "\"න\", \"ඳ\",\"ප\", \"ඵ\", \"බ\", \"භ\", \"ම\", \"ඹ\", \"ය\", \"ර\", \"ල\", \"ව\",\"ශ\", \"ෂ\", \"ස\", \"හ\", \"ළ\", \"ෆ\",\"෴\", \"\\u200d\"]\n",
    "\n",
    "#\\u200d is known as the ZERO WIDTH JOINER. It is required in special cases when using Sinhala language.\n",
    "sinhalaVowels = [\"්\", \"ා\", \"ැ\", \"ෑ\", \"ි\", \"ී\", \"ු\", \"ූ\", \"ෘ\", \"ෙ\", \"ේ\", \"ෛ\", \"ො\", \"ෝ\",\"ෞ\", \"ෟ\", \"ෲ\", \"ෳ\", \"ර්‍\"]\n",
    "\n",
    "#A dictonary created to validate the vowel errors in the texts\n",
    "vowelsFixed = {\"ෙ\" + \"්\": \"ේ\", \"්\" + \"ෙ\": \"ේ\", \"ෙ\" + \"ා\": \"ො\", \"ා\" + \"ෙ\": \"ො\", \"ේ\" + \"ා\": \"ෝ\", \"ො\" + \"්\": \"ෝ\", \"ෙෙ\": \"ෛ\",\n",
    "\"ෘෘ\": \"ෲ\", \"ෙ\" + \"ෟ\": \"ෞ\", \"ෟ\" + \"ෙ\": \"ෞ\", \"ි\" + \"ී\": \"ී\", \"ී\" + \"ි\": \"ී\", \"ේ\" + \"්\": \"ේ\", \"ේ\" + \"ෙ\": \"ේ\", \"ො\" + \"ා\": \"ො\",\n",
    "\"ො\" + \"ෙ\": \"ො\", \"ෝ\" + \"ා\": \"ෝ\", \"ෝ\" + \"්\": \"ෝ\", \"ෝ\" + \"ෙ\": \"ෝ\", \"ෝ\" + \"ේ\": \"ෝ\", \"ෝ\" + \"ො\": \"ෝ\", \"ෞ\" + \"ෟ\": \"ෞ\",\n",
    "\"ෞ\" + \"ෙ\": \"ෞ\", \"ො\" + \"ෟ\": \"ෞ\", \"ෟ\" + \"ො\": \"ෞ\",}\n",
    "\n",
    "#A dictonary created to simplify special characters used in sinhala words to minimize the variety\n",
    "simplifiedChars = {\"ඛ\": \"ක\", \"ඝ\": \"ග\", \"ඟ\": \"ග\", \"ඡ\": \"ච\", \"ඣ\": \"ජ\", \"ඦ\": \"ජ\", \"ඤ\": \"ඥ\", \"ඨ\": \"ට\", \"ඪ\": \"ඩ\", \"ණ\": \"න\",\n",
    "\"ඳ\": \"ද\", \"ඵ\": \"ප\", \"භ\": \"බ\", \"ඹ\": \"බ\", \"ශ\": \"ෂ\", \"ළ\": \"ල\", \"ආ\": \"අ\", \"ඈ\": \"ඇ\", \"ඊ\": \"ඉ\", \"ඌ\": \"උ\", \"ඒ\": \"එ\", \"ඕ\": \"ඔ\",\n",
    "\"ා\": \"\", \"ෑ\": \"ැ\", \"ී\": \"ි\", \"ූ\": \"ු\", \"ේ\": \"ෙ\", \"ෝ\": \"ො\", \"ෲ\": \"ෘ\"}\n",
    "\n",
    "def isSinhalaLetter(char: str) -> bool:\n",
    "  return char in sinhalaChars\n",
    "\n",
    "def isSinhalaVowel(char: str) -> bool:\n",
    "  return char in sinhalaVowels\n",
    "\n",
    "def getFixedVowel(vowel: str) -> str:\n",
    "  return vowelsFixed[vowel]\n",
    "\n",
    "def getSimplifiedChar(character: str) -> str:\n",
    "  if len(character) != 1:\n",
    "    raise TypeError(\"character should be a string with length 1\")\n",
    "  try:\n",
    "    return simplifiedChars[character]\n",
    "  except KeyError:\n",
    "    return character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuzyNlEeCgDz"
   },
   "outputs": [],
   "source": [
    "def replaceUrl(text: str) -> str:\n",
    "  return re.sub(r'(http://www\\.|https://www\\.|http://|https://)[a-z0-9]+([\\-.]{1}[a-z0-9A-Z/]+)*', '', text)\n",
    "\n",
    "def removeRetweetState(text: str) -> str:\n",
    "  return re.sub(r'^RT @\\w*: ', '', text)\n",
    "\n",
    "def replaceMention(text: str) -> str:\n",
    "  return re.sub(r'@\\w*', '', text)\n",
    "\n",
    "def splitTokens(text: str) -> list:\n",
    "  emojis = ''.join(emj for emj in emoji.EMOJI_DATA.keys())\n",
    "  return [token for token in re.split(r'[.…,‌ ¸‚\\\"/|—¦”‘\\'“’´!@#$%^&*+\\-£?˜()\\[\\]{\\}:;–Ê  �‪‬‏0123456789' + emojis + ']', text) if token != \"\"]\n",
    "\n",
    "def setSpacesAmongEmojis(text: str) -> str:\n",
    "  modified_text = \"\"\n",
    "  for c in text:\n",
    "    modified_text += c\n",
    "    if c in emoji.UNICODE_EMOJI:\n",
    "      modified_text += \" \"\n",
    "  return modified_text\n",
    "\n",
    "def simplifySinhalaText(text: str) -> str:\n",
    "  modified_text = \"\"\n",
    "  for c in text:\n",
    "    modified_text += getSimplifiedChar(c)\n",
    "  return modified_text\n",
    "\n",
    "def stemWord(word: str) -> str:\n",
    "  if len(word) < 4:\n",
    "    return word\n",
    "  # remove 'ට'\n",
    "  if word[-1] == 'ට':\n",
    "    return word[:-1]\n",
    "  # remove 'ද'\n",
    "  if word[-1] == 'ද':\n",
    "    return word[:-1]\n",
    "  # remove 'ටත්'\n",
    "  if word[-3:] == 'ටත්':\n",
    "    return word[:-3]\n",
    "  # remove 'එක්'\n",
    "  if word[-3:] == 'ෙක්':\n",
    "    return word[:-3]\n",
    "  # remove 'එ'\n",
    "  if word[-1:] == 'ෙ':\n",
    "    return word[:-1]\n",
    "  # remove 'ක්'\n",
    "  if word[-2:] == 'ක්':\n",
    "    return word[:-2]\n",
    "  # remove 'ගෙ' (instead of ගේ because this step comes after simplifying text)\n",
    "  if word[-2:] == 'ගෙ':\n",
    "    return word[:-2]\n",
    "  # else\n",
    "  return word\n",
    "\n",
    "def filterText(text: str) -> list:\n",
    "  return [stemWord(token) for token in splitTokens(replaceUrl(replaceMention(simplifySinhalaText(removeRetweetState(text.strip('\"')).lower()))))]\n",
    "\n",
    "def simplifySinhala(rawtext: str) -> str:\n",
    "    simplifiedList = filterText(rawtext)\n",
    "    simplifiedStr = \"\"\n",
    "\n",
    "    for word in simplifiedList:\n",
    "        simplifiedStr = simplifiedStr + word + \" \"\n",
    "\n",
    "    return simplifiedStr.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAe5qv5cClLq"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/RusiruWijethilake/DepFlow/main/dataset.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "labels = df['label']\n",
    "\n",
    "for sent in df['text']:\n",
    "  sentences.append(simplifySinhala(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdiTus6YCnYj"
   },
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4taRCU4DSM0"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2urRUCZDUUy"
   },
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWt8hpjQDbrV",
    "outputId": "6423a2bd-a5d2-4724-d5fc-73329402d709"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(24, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fC8dmLHZDhKZ",
    "outputId": "1055e6f4-eec2-4575-ea32-93ea0a6f91db"
   },
   "outputs": [],
   "source": [
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "QJBx95cdDpHh",
    "outputId": "1ce35a98-c897-4b47-db51-e5a3ee021ad1"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ynoe9i43Dsud",
    "outputId": "e1b1c30f-5d95-4b21-f8c0-fa1bcfe0f549"
   },
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_sentence(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_sentence(training_padded[0]))\n",
    "print(training_sentences[2])\n",
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXAnfVYdDuyg",
    "outputId": "bbe95c69-8a74-41d0-a7df-a7f5c88eb446"
   },
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZost6BTDw5X"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "obfCloMDD1oA",
    "outputId": "4d53c0d2-bfd5-4ced-9fe9-65e41297d5e9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/depflow_model_trained')\n",
    "model.save('depflow_trained_model.h5')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iu2b0LxhZtQm",
    "outputId": "5358092f-9703-4a2f-8d04-903bb1c07317"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "795Kb4N-D3Qa",
    "outputId": "82729f68-f54d-48a0-a75d-88f01dc5816f"
   },
   "outputs": [],
   "source": [
    "sentence = [simplifySinhala(\"මට ඇති මේ දුක දරන් හිටියා.\"), simplifySinhala(\"මට සතුටුයි හොදටම\")]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItmrbW56JTQJ",
    "outputId": "fbaad5a9-befc-4329-b20e-4304fb1d7f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step\n",
      "මට ඇති මේ දුක දරන් හිටියා.  : is a depressive post\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "මම මැරිලම යන්නම්  : is a depressive post\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "මම මරිලා ගියාම හැමොටම හොදයි.  : is a depressive post\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "දුක කියන්නෙ හෙට වැඩට යන්න තිබීමයි  : is a depressive post\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "අපි හෙට උදේට මොනවද කන්නේ?  : is a depressive post\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "ඇයි මට මෙච්චර දුකක් දෙන්නේ දෙවියනේ  : is a depressive post\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "ඔයාටනම් ඉතින් හිනා  : is not a depressive post\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "මම හිතන්නේ මට තනිකම දැනෙනවා වැඩී  : is a depressive post\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "mata godak dukayi  : is a depressive post\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "I feel like I want to cry  : is a depressive post\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "මම මේ දවස් ටිකේම දුක දරාගන්න බැරුව හොදටම ඇඩුවා  : is a depressive post\n"
     ]
    },
    {
     "data": {
      "text/plain": "CompletedProcess(args='pbcopy', returncode=0)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_depressive(post: str):\n",
    "  post_sequence = tokenizer.texts_to_sequences(simplifySinhala(post))\n",
    "  padded_post_sequence = pad_sequences(post_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "  post_prediction = model.predict(padded_post_sequence)\n",
    "  label = post_prediction.max().round()\n",
    "  if label >= 1 :\n",
    "    print(post, \" : is a depressive post\")\n",
    "  else:\n",
    "    print(post, \" : is not a depressive post\")\n",
    "\n",
    "check_depressive(\"මට ඇති මේ දුක දරන් හිටියා.\")\n",
    "check_depressive(\"මම මැරිලම යන්නම්\")\n",
    "check_depressive(\"මම මරිලා ගියාම හැමොටම හොදයි.\")\n",
    "check_depressive(\"දුක කියන්නෙ හෙට වැඩට යන්න තිබීමයි\")\n",
    "check_depressive(\"අපි හෙට උදේට මොනවද කන්නේ?\")\n",
    "check_depressive('ඇයි මට මෙච්චර දුකක් දෙන්නේ දෙවියනේ')\n",
    "check_depressive('ඔයාටනම් ඉතින් හිනා')\n",
    "check_depressive('මම හිතන්නේ මට තනිකම දැනෙනවා වැඩී')\n",
    "check_depressive('mata godak dukayi')\n",
    "check_depressive('I feel like I want to cry')\n",
    "check_depressive('මම මේ දවස් ටිකේම දුක දරාගන්න බැරුව හොදටම ඇඩුවා')\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\"pbcopy\", text=True, input=str(word_index))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
